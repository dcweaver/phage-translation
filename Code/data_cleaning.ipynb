{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook summary\n",
    "\n",
    "The purpose of this notebook is simply to clean up some of the files that were created in `Compile_data.ipynb`. The output of that notebook was a set of `.tsv` files organized into folders within `../Data/` for both host and virus genomes. Those `.tsv` files (potentially) contained some bugs so we're going to run some filters/tests and store cleaned versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "The only custom import here is iCUB (a codon usage bias calculator) that can be found here:\n",
    "https://github.com/amarallab/iCUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('../../iCUB/') ###iCUB needs to be installed for this to work\n",
    "import iCUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions\n",
    "\n",
    "Despite some similarities, there are a few crucial differences between the host and virus `.tsv` files that dictate separate cleaning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_cleaning(df):\n",
    "    \n",
    "    df = df[df['upstream_sequence'].isnull()==False]\n",
    "    df = df[df['coding_sequence'].isnull()==False]\n",
    "    \n",
    "    ###Add a codon usage bias and GC percent columns\n",
    "    for index in df.index:\n",
    "        nt_seq = df.at[index, \"coding_sequence\"]\n",
    "        if len(nt_seq) != nt_seq.count('A') + nt_seq.count('T') + nt_seq.count('C') + nt_seq.count('G'):\n",
    "            continue\n",
    "        if len(nt_seq)%3 != 0:\n",
    "            continue\n",
    "        df.at[index, 'iCUB'] = iCUB.iCUB_Calculator(nt_seq).get_iCUB()\n",
    "        #\n",
    "        df.at[index, 'GC_cds'] = (nt_seq.count('G') + nt_seq.count('C')) / len(nt_seq)\n",
    "        #\n",
    "        up_seq = df.at[index, 'upstream_sequence']\n",
    "        df.at[index, 'GC_upstream'] = (up_seq.count('G') + up_seq.count('C')) / len(up_seq)\n",
    "\n",
    "\n",
    "    \n",
    "    ###Remove genes for which we were unable to calculate codon usage bias or RBS energies\n",
    "    df = df[df['iCUB'].isnull()==False]\n",
    "    df = df[df['GC_cds'].isnull()==False]\n",
    "    df = df[df['GC_upstream'].isnull()==False]\n",
    "    df = df[df['RBS_energy'].isnull()==False]\n",
    "    df = df[df['RBS_energy_upstream'].isnull()==False]\n",
    "    return df\n",
    "\n",
    "def clean_host_tsv(df):\n",
    "    \"\"\"\n",
    "    WRITE A BRIEF PURPOSE/SUMMARY\n",
    "    \n",
    "    Development notes: I could also think about testing for stop codons within coding sequences\n",
    "    and filtering accordingly. Also a way to test for possible non-standard genetic code usage\n",
    "    \n",
    "    Input/s:\n",
    "        df - a pandas dataframe with numeric indices, outputted from Compile_data.ipynb and read\n",
    "                back in\n",
    "        \n",
    "    Output/s:\n",
    "        df - a clean version of the dataframe with one new column (iCUB) and (potentially)\n",
    "                several rows removed\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    ###Filter out possible prophage genes by removing anything involving the word phage\n",
    "    ###Numerous possibilities/ways to do this and not all genomes might have any decent\n",
    "    ###descriptions in the qualifiers.\n",
    "    filter_word = 'phage'\n",
    "    df = df[(df['qualifiers'].str.contains(filter_word)==False)]\n",
    "    \n",
    "    df = common_cleaning(df)\n",
    "    \n",
    "    ###Now ensure that each locus tag is only used once and when in doubt remove them both\n",
    "    df['locus_tag'] = df['qualifiers'].str.split('locus_tag=', n=1, expand=True)[1]\\\n",
    "                            .str.split(';', n=1, expand=True)[0]\n",
    "    df = df.drop_duplicates(subset = ['locus_tag'], keep = False)\n",
    "    return df\n",
    "\n",
    "def clean_virus_tsv(df):\n",
    "    \"\"\"\n",
    "    This is the same basic structure as the \"clean_host_tsv\" function. \n",
    "    \n",
    "    Input/s:\n",
    "        df - a pandas dataframe with numeric indices, outputted from Compile_data.ipynb and read\n",
    "                back in\n",
    "        \n",
    "    Output/s:\n",
    "        df - a clean version of the dataframe with one new column (iCUB) and (potentially)\n",
    "                several rows removed\n",
    "                \n",
    "    \"\"\"  \n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    df = common_cleaning(df)\n",
    "\n",
    "    ###Now ensure that each viral_id tag is only used once (and when in doubt remove them BOTH)\n",
    "    df['viral_id'] = df['qualifiers'].str.split('ID=', n=1, expand=True)[1]\\\n",
    "                            .str.split(';', n=1, expand=True)[0]\n",
    "    df = df.drop_duplicates(subset = [\"viral_id\"], keep = False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, test the translation table assumption\n",
    "\n",
    "In the future, if this analysis is extended to any genome where the `transl_table` qualifier is **not** 11 there will need to be extensive code re-factoring, starting with re-annotating the viral genomes to account for this fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/host_genomes/28450.tsv\n",
      "5727 [('11', 5727)]\n",
      "../Data/host_genomes/1590.tsv\n",
      "3013 [('11', 3013)]\n",
      "../Data/host_genomes/562.tsv\n",
      "4379 [('11', 4379)]\n",
      "../Data/host_genomes/357276.tsv\n",
      "4343 [('11', 4343)]\n",
      "../Data/host_genomes/657318.tsv\n",
      "3294 [('11', 3294)]\n",
      "../Data/host_genomes/573.tsv\n",
      "5316 [('11', 5316)]\n",
      "../Data/host_genomes/1280.tsv\n",
      "2767 [('11', 2767)]\n",
      "../Data/host_genomes/305.tsv\n",
      "3466 [('11', 3466)]\n",
      "../Data/host_genomes/435591.tsv\n",
      "3979 [('11', 3979)]\n",
      "../Data/host_genomes/470.tsv\n",
      "4327 [('11', 4327)]\n",
      "../Data/host_genomes/287.tsv\n",
      "5573 [('11', 5573)]\n",
      "../Data/host_genomes/1314.tsv\n",
      "1693 [('11', 1690)]\n",
      "../Data/host_genomes/36809.tsv\n",
      "4920 [('11', 4920)]\n",
      "../Data/host_genomes/717959.tsv\n",
      "3110 [('11', 3110)]\n",
      "../Data/host_genomes/90371.tsv\n",
      "4447 [('11', 4447)]\n",
      "../Data/host_genomes/1639.tsv\n",
      "2867 [('11', 2867)]\n",
      "../Data/host_genomes/1428.tsv\n",
      "5117 [('11', 5117)]\n"
     ]
    }
   ],
   "source": [
    "for host_tsv_file in glob.glob('../Data/host_genomes/' + '*.tsv')[:]:\n",
    "    if '.clean.' in host_tsv_file:\n",
    "        continue\n",
    "    print(host_tsv_file)\n",
    "    df = pd.read_csv(host_tsv_file, sep = \"\\t\", index_col = 0)\n",
    "    df['transl_table'] = df['qualifiers'].str.split('transl_table=', n=1, expand=True)[1]\\\n",
    "                        .str.split(';', n=1, expand=True)[0]\n",
    "    print(df.shape[0], list(df['transl_table'].value_counts().items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cleaning function on all host `.tsv` files\n",
    "\n",
    "This isn't fast but it's also not optimized for speed. At the moment I am only looking at < 20 host files and this code only needs to be run once so speed is taking a backseat to thoroughness. Ergo, for loops, etc. As it stands, my guess is that this cell takes ~45 mins or so to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_tsv_file in glob.glob('../Data/host_genomes/' + '*.tsv'):\n",
    "    if '.clean.' in host_tsv_file:\n",
    "        continue\n",
    "    print(host_tsv_file)\n",
    "    ###\n",
    "    df = pd.read_csv(host_tsv_file, sep = \"\\t\", index_col = 0)\n",
    "    initial_shape = df.shape\n",
    "    df = clean_host_tsv(df)\n",
    "    final_shape = df.shape\n",
    "    ###\n",
    "    gene_ratio = final_shape[0]/initial_shape[0]\n",
    "    print(gene_ratio)\n",
    "    if gene_ratio <= 0.8:\n",
    "        break\n",
    "    ###\n",
    "    clean_tsv_loc = host_tsv_file.replace(\".tsv\",\".clean.tsv\")\n",
    "    df.to_csv(clean_tsv_loc, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cleaning function on all phage `.tsv` files\n",
    "\n",
    "Currently printing a lot but realistically there aren't to many files in my current database so it doesn't seem to be a problem to monitor ongoing progress in this manner. This runs fairly quickly, ~30 mins I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for virus_folder in glob.glob('../Data/*_rep_viruses/'):\n",
    "    print('###', virus_folder)\n",
    "    for virus_tsv_file in glob.glob(virus_folder + '*.tsv'):\n",
    "        if \".clean.\" in virus_tsv_file:\n",
    "            continue\n",
    "        print(virus_tsv_file)\n",
    "        ###\n",
    "        df = pd.read_csv(virus_tsv_file, sep = \"\\t\", index_col = 0)\n",
    "        initial_shape = df.shape\n",
    "        df = clean_virus_tsv(df)\n",
    "        final_shape = df.shape\n",
    "        ###\n",
    "        gene_ratio = final_shape[0]/initial_shape[0]\n",
    "        if gene_ratio <= 0.8:\n",
    "            print('Strange case', gene_ratio)\n",
    "            break\n",
    "        ###\n",
    "        clean_tsv_loc = virus_tsv_file.replace(\".tsv\",\".clean.tsv\")\n",
    "        df.to_csv(clean_tsv_loc, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
